{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Securing Federated Learning\n",
    "\n",
    "- Lesson 1: Trusted Aggregator\n",
    "- Lesson 2: Intro to Additive Secret Sharing\n",
    "- Lesson 3: Intro to Fixed Precision Encoding\n",
    "- Lesson 4: Secret Sharing + Fixed Precision in PySyft\n",
    "- Final Project: Federated Learning wtih Encrypted Gradient Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:00:15.676261Z",
     "start_time": "2019-07-05T15:59:38.913131Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.13.1). Fix this by compiling custom ops.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dists\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import syft\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "hook = syft.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Federated Learning with a Trusted Aggregator\n",
    "\n",
    "In the last section, we learned how to train a model on a distributed dataset using Federated Learning. In particular, the last project aggregated gradients directly from one data owner to another. \n",
    "\n",
    "However, while in some cases it could be ideal to do this, what would be even better is to be able to choose a neutral third party to perform the aggregation.\n",
    "\n",
    "As it turns out, we can use the same tools we used previously to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Federated Learning with a Trusted Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:00:15.683253Z",
     "start_time": "2019-07-05T16:00:15.678293Z"
    }
   },
   "outputs": [],
   "source": [
    "n_workers = 30\n",
    "\n",
    "workers = [syft.VirtualWorker(hook, id=\"Worker:{:d}\".format(i)) for i in range(n_workers)]\n",
    "trusted_aggregator = syft.VirtualWorker(hook, id='Trusted Aggregator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:00:24.949868Z",
     "start_time": "2019-07-05T16:00:15.689264Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset  = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "federated_mnist_trainset = mnist_trainset.federate(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:00:24.960839Z",
     "start_time": "2019-07-05T16:00:24.951862Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "\n",
    "        # 1x28x28\n",
    "        self.conv0      = nn.Conv2d(1, 4, 3, padding=1)\n",
    "        self.maxpool0   = nn.MaxPool2d(2)\n",
    "        # 4x14x14\n",
    "        self.conv1      = nn.Conv2d(4, 6, 3, padding=1)\n",
    "        self.maxpool1   = nn.MaxPool2d(2)\n",
    "        # 6x 7x 7\n",
    "        self.conv2      = nn.Conv2d(6, 8, 3, padding=1)\n",
    "        self.maxpool2   = nn.MaxPool2d(2, padding=1)\n",
    "        # 8x 4x 4 = 128\n",
    "        self.fc         = nn.Linear(128, 10)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.fc(x.view(-1, 128))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged Gradients Approach (My Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:00:24.971844Z",
     "start_time": "2019-07-05T16:00:24.964828Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MNISTClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:13:16.110952Z",
     "start_time": "2019-07-05T16:00:24.972816Z"
    },
    "code_folding": [
     8
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "worker 29/29\n",
      "    Training Loss: 2.309325695037842\n",
      "    Training Accuracy: 0.08124999701976776\n",
      "Epoch 1:\n",
      "worker 29/29\n",
      "    Training Loss: 2.297940731048584\n",
      "    Training Accuracy: 0.12994791567325592\n",
      "Epoch 2:\n",
      "worker 29/29\n",
      "    Training Loss: 2.2785115242004395\n",
      "    Training Accuracy: 0.3033854067325592\n",
      "Epoch 3:\n",
      "worker 29/29\n",
      "    Training Loss: 2.246328115463257\n",
      "    Training Accuracy: 0.3213541805744171\n",
      "Epoch 4:\n",
      "worker 29/29\n",
      "    Training Loss: 2.179736375808716\n",
      "    Training Accuracy: 0.3309895694255829\n",
      "Epoch 5:\n",
      "worker 29/29\n",
      "    Training Loss: 2.0554521083831787\n",
      "    Training Accuracy: 0.42005208134651184\n",
      "Epoch 6:\n",
      "worker 29/29\n",
      "    Training Loss: 1.8949917554855347\n",
      "    Training Accuracy: 0.500781238079071\n",
      "Epoch 7:\n",
      "worker 29/29\n",
      "    Training Loss: 1.6481525897979736\n",
      "    Training Accuracy: 0.6005208492279053\n",
      "Epoch 8:\n",
      "worker 29/29\n",
      "    Training Loss: 1.3638745546340942\n",
      "    Training Accuracy: 0.6244791746139526\n",
      "Epoch 9:\n",
      "worker 29/29\n",
      "    Training Loss: 1.1782138347625732\n",
      "    Training Accuracy: 0.6416666507720947\n",
      "Epoch 10:\n",
      "worker 29/29\n",
      "    Training Loss: 1.0240551233291626\n",
      "    Training Accuracy: 0.6596354246139526\n",
      "Epoch 11:\n",
      "worker 29/29\n",
      "    Training Loss: 0.9599606394767761\n",
      "    Training Accuracy: 0.6606770753860474\n",
      "Epoch 12:\n",
      "worker 29/29\n",
      "    Training Loss: 0.9000529646873474\n",
      "    Training Accuracy: 0.6880208253860474\n",
      "Epoch 13:\n",
      "worker 29/29\n",
      "    Training Loss: 0.7802532315254211\n",
      "    Training Accuracy: 0.7567708492279053\n",
      "Epoch 14:\n",
      "worker 29/29\n",
      "    Training Loss: 0.7666501402854919\n",
      "    Training Accuracy: 0.7635416388511658\n",
      "Epoch 15:\n",
      "worker 29/29\n",
      "    Training Loss: 0.6716976165771484\n",
      "    Training Accuracy: 0.7763020992279053\n",
      "Epoch 16:\n",
      "worker 29/29\n",
      "    Training Loss: 0.6115378141403198\n",
      "    Training Accuracy: 0.8031250238418579\n",
      "Epoch 17:\n",
      "worker 29/29\n",
      "    Training Loss: 0.5946950316429138\n",
      "    Training Accuracy: 0.8070312738418579\n",
      "Epoch 18:\n",
      "worker 29/29\n",
      "    Training Loss: 0.5361897945404053\n",
      "    Training Accuracy: 0.83203125\n",
      "Epoch 19:\n",
      "worker 29/29\n",
      "    Training Loss: 0.5293868184089661\n",
      "    Training Accuracy: 0.8216145634651184\n",
      "Epoch 20:\n",
      "worker 29/29\n",
      "    Training Loss: 0.5052085518836975\n",
      "    Training Accuracy: 0.8343750238418579\n",
      "Epoch 21:\n",
      "worker 29/29\n",
      "    Training Loss: 0.4725129306316376\n",
      "    Training Accuracy: 0.8489583134651184\n",
      "Epoch 22:\n",
      "worker 29/29\n",
      "    Training Loss: 0.4249837100505829\n",
      "    Training Accuracy: 0.862500011920929\n",
      "Epoch 23:\n",
      "worker 29/29\n",
      "    Training Loss: 0.4356556832790375\n",
      "    Training Accuracy: 0.8638020753860474\n",
      "Epoch 24:\n",
      "worker 29/29\n",
      "    Training Loss: 0.4327811896800995\n",
      "    Training Accuracy: 0.8648437261581421\n",
      "Epoch 25:\n",
      "worker 29/29\n",
      "    Training Loss: 0.38031283020973206\n",
      "    Training Accuracy: 0.8768228888511658\n",
      "Epoch 26:\n",
      "worker 29/29\n",
      "    Training Loss: 0.35972079634666443\n",
      "    Training Accuracy: 0.8908854126930237\n",
      "Epoch 27:\n",
      "worker 29/29\n",
      "    Training Loss: 0.3543413281440735\n",
      "    Training Accuracy: 0.8864583373069763\n",
      "Epoch 28:\n",
      "worker 29/29\n",
      "    Training Loss: 0.33362218737602234\n",
      "    Training Accuracy: 0.8973958492279053\n",
      "Epoch 29:\n",
      "worker 29/29\n",
      "    Training Loss: 0.3378082513809204\n",
      "    Training Accuracy: 0.8971354365348816\n",
      "Epoch 30:\n",
      "worker 29/29\n",
      "    Training Loss: 0.3293657898902893\n",
      "    Training Accuracy: 0.9018229246139526\n",
      "Epoch 31:\n",
      "worker 29/29\n",
      "    Training Loss: 0.3175329864025116\n",
      "    Training Accuracy: 0.9044271111488342\n",
      "Epoch 32:\n",
      "worker 29/29\n",
      "    Training Loss: 0.28738996386528015\n",
      "    Training Accuracy: 0.9046875238418579\n",
      "Epoch 33:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2749990224838257\n",
      "    Training Accuracy: 0.9145833253860474\n",
      "Epoch 34:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2692655324935913\n",
      "    Training Accuracy: 0.9190104007720947\n",
      "Epoch 35:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2768809497356415\n",
      "    Training Accuracy: 0.9200521111488342\n",
      "Epoch 36:\n",
      "worker 29/29\n",
      "    Training Loss: 0.25843337178230286\n",
      "    Training Accuracy: 0.9229166507720947\n",
      "Epoch 37:\n",
      "worker 29/29\n",
      "    Training Loss: 0.24873189628124237\n",
      "    Training Accuracy: 0.9177083373069763\n",
      "Epoch 38:\n",
      "worker 29/29\n",
      "    Training Loss: 0.23183989524841309\n",
      "    Training Accuracy: 0.9283854365348816\n",
      "Epoch 39:\n",
      "worker 29/29\n",
      "    Training Loss: 0.24566669762134552\n",
      "    Training Accuracy: 0.9244791865348816\n",
      "Epoch 40:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2377440333366394\n",
      "    Training Accuracy: 0.9281250238418579\n",
      "Epoch 41:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2301674783229828\n",
      "    Training Accuracy: 0.9309895634651184\n",
      "Epoch 42:\n",
      "worker 29/29\n",
      "    Training Loss: 0.22104033827781677\n",
      "    Training Accuracy: 0.9283854365348816\n",
      "Epoch 43:\n",
      "worker 29/29\n",
      "    Training Loss: 0.19993247091770172\n",
      "    Training Accuracy: 0.9375\n",
      "Epoch 44:\n",
      "worker 29/29\n",
      "    Training Loss: 0.21669787168502808\n",
      "    Training Accuracy: 0.934374988079071\n",
      "Epoch 45:\n",
      "worker 29/29\n",
      "    Training Loss: 0.20263952016830444\n",
      "    Training Accuracy: 0.9346354007720947\n",
      "Epoch 46:\n",
      "worker 29/29\n",
      "    Training Loss: 0.21915745735168457\n",
      "    Training Accuracy: 0.9354166388511658\n",
      "Epoch 47:\n",
      "worker 29/29\n",
      "    Training Loss: 0.19051401317119598\n",
      "    Training Accuracy: 0.9432291388511658\n",
      "Epoch 48:\n",
      "worker 29/29\n",
      "    Training Loss: 0.19315379858016968\n",
      "    Training Accuracy: 0.9375\n",
      "Epoch 49:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1910366266965866\n",
      "    Training Accuracy: 0.9395833611488342\n",
      "Epoch 50:\n",
      "worker 29/29\n",
      "    Training Loss: 0.19567842781543732\n",
      "    Training Accuracy: 0.9388020634651184\n",
      "Epoch 51:\n",
      "worker 29/29\n",
      "    Training Loss: 0.17362888157367706\n",
      "    Training Accuracy: 0.9463541507720947\n",
      "Epoch 52:\n",
      "worker 29/29\n",
      "    Training Loss: 0.18183094263076782\n",
      "    Training Accuracy: 0.9450520873069763\n",
      "Epoch 53:\n",
      "worker 29/29\n",
      "    Training Loss: 0.18575914204120636\n",
      "    Training Accuracy: 0.9424479007720947\n",
      "Epoch 54:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1726188063621521\n",
      "    Training Accuracy: 0.9466145634651184\n",
      "Epoch 55:\n",
      "worker 29/29\n",
      "    Training Loss: 0.17082366347312927\n",
      "    Training Accuracy: 0.9463541507720947\n",
      "Epoch 56:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1630382239818573\n",
      "    Training Accuracy: 0.9489583373069763\n",
      "Epoch 57:\n",
      "worker 29/29\n",
      "    Training Loss: 0.16875429451465607\n",
      "    Training Accuracy: 0.9497395753860474\n",
      "Epoch 58:\n",
      "worker 29/29\n",
      "    Training Loss: 0.16154974699020386\n",
      "    Training Accuracy: 0.94921875\n",
      "Epoch 59:\n",
      "worker 29/29\n",
      "    Training Loss: 0.153272807598114\n",
      "    Training Accuracy: 0.9557291865348816\n",
      "Epoch 60:\n",
      "worker 29/29\n",
      "    Training Loss: 0.16403000056743622\n",
      "    Training Accuracy: 0.9518229365348816\n",
      "Epoch 61:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1629384458065033\n",
      "    Training Accuracy: 0.9453125\n",
      "Epoch 62:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12787003815174103\n",
      "    Training Accuracy: 0.9598958492279053\n",
      "Epoch 63:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1501680165529251\n",
      "    Training Accuracy: 0.9528645873069763\n",
      "Epoch 64:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1635431945323944\n",
      "    Training Accuracy: 0.9442708492279053\n",
      "Epoch 65:\n",
      "worker 29/29\n",
      "    Training Loss: 0.13835635781288147\n",
      "    Training Accuracy: 0.9541666507720947\n",
      "Epoch 66:\n",
      "worker 29/29\n",
      "    Training Loss: 0.13424643874168396\n",
      "    Training Accuracy: 0.9572916626930237\n",
      "Epoch 67:\n",
      "worker 29/29\n",
      "    Training Loss: 0.15332259237766266\n",
      "    Training Accuracy: 0.9505208134651184\n",
      "Epoch 68:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1280469000339508\n",
      "    Training Accuracy: 0.960156261920929\n",
      "Epoch 69:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1366317719221115\n",
      "    Training Accuracy: 0.9565104246139526\n",
      "Epoch 70:\n",
      "worker 29/29\n",
      "    Training Loss: 0.14059846103191376\n",
      "    Training Accuracy: 0.95703125\n",
      "Epoch 71:\n",
      "worker 29/29\n",
      "    Training Loss: 0.13324716687202454\n",
      "    Training Accuracy: 0.9572916626930237\n",
      "Epoch 72:\n",
      "worker 29/29\n",
      "    Training Loss: 0.16117843985557556\n",
      "    Training Accuracy: 0.9494791626930237\n",
      "Epoch 73:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1369345486164093\n",
      "    Training Accuracy: 0.9559895992279053\n",
      "Epoch 74:\n",
      "worker 29/29\n",
      "    Training Loss: 0.13296537101268768\n",
      "    Training Accuracy: 0.9541666507720947\n",
      "Epoch 75:\n",
      "worker 29/29\n",
      "    Training Loss: 0.13966795802116394\n",
      "    Training Accuracy: 0.9554687738418579\n",
      "Epoch 76:\n",
      "worker 29/29\n",
      "    Training Loss: 0.11250496655702591\n",
      "    Training Accuracy: 0.9624999761581421\n",
      "Epoch 77:\n",
      "worker 29/29\n",
      "    Training Loss: 0.11484067142009735\n",
      "    Training Accuracy: 0.9645833373069763\n",
      "Epoch 78:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12599687278270721\n",
      "    Training Accuracy: 0.9630208611488342\n",
      "Epoch 79:\n",
      "worker 29/29\n",
      "    Training Loss: 0.14007194340229034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Training Accuracy: 0.9557291865348816\n",
      "Epoch 80:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12067414820194244\n",
      "    Training Accuracy: 0.9611979126930237\n",
      "Epoch 81:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12396706640720367\n",
      "    Training Accuracy: 0.964062511920929\n",
      "Epoch 82:\n",
      "worker 29/29\n",
      "    Training Loss: 0.10720615833997726\n",
      "    Training Accuracy: 0.9664062261581421\n",
      "Epoch 83:\n",
      "worker 29/29\n",
      "    Training Loss: 0.13940660655498505\n",
      "    Training Accuracy: 0.9557291865348816\n",
      "Epoch 84:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12093702703714371\n",
      "    Training Accuracy: 0.9614583253860474\n",
      "Epoch 85:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12062196433544159\n",
      "    Training Accuracy: 0.9598958492279053\n",
      "Epoch 86:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1198357492685318\n",
      "    Training Accuracy: 0.9635416865348816\n",
      "Epoch 87:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12029659003019333\n",
      "    Training Accuracy: 0.9604166746139526\n",
      "Epoch 88:\n",
      "worker 29/29\n",
      "    Training Loss: 0.11127721518278122\n",
      "    Training Accuracy: 0.9619791507720947\n",
      "Epoch 89:\n",
      "worker 29/29\n",
      "    Training Loss: 0.126383438706398\n",
      "    Training Accuracy: 0.9630208611488342\n",
      "Epoch 90:\n",
      "worker 29/29\n",
      "    Training Loss: 0.10522580146789551\n",
      "    Training Accuracy: 0.9677083492279053\n",
      "Epoch 91:\n",
      "worker 29/29\n",
      "    Training Loss: 0.11017360538244247\n",
      "    Training Accuracy: 0.9638020992279053\n",
      "Epoch 92:\n",
      "worker 29/29\n",
      "    Training Loss: 0.112601637840271\n",
      "    Training Accuracy: 0.96484375\n",
      "Epoch 93:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1264674812555313\n",
      "    Training Accuracy: 0.9627603888511658\n",
      "Epoch 94:\n",
      "worker 29/29\n",
      "    Training Loss: 0.1269432008266449\n",
      "    Training Accuracy: 0.9624999761581421\n",
      "Epoch 95:\n",
      "worker 29/29\n",
      "    Training Loss: 0.11714613437652588\n",
      "    Training Accuracy: 0.9614583253860474\n",
      "Epoch 96:\n",
      "worker 29/29\n",
      "    Training Loss: 0.10763761401176453\n",
      "    Training Accuracy: 0.9690104126930237\n",
      "Epoch 97:\n",
      "worker 29/29\n",
      "    Training Loss: 0.09475018084049225\n",
      "    Training Accuracy: 0.9713541865348816\n",
      "Epoch 98:\n",
      "worker 29/29\n",
      "    Training Loss: 0.11164319515228271\n",
      "    Training Accuracy: 0.9643229246139526\n",
      "Epoch 99:\n",
      "worker 29/29\n",
      "    Training Loss: 0.12078645825386047\n",
      "    Training Accuracy: 0.9624999761581421\n"
     ]
    }
   ],
   "source": [
    "n_epochs   = 100\n",
    "lr         = 2e-2\n",
    "batch_size = 128\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "stored_grads = OrderedDict((name, param.new_zeros(param.shape).send(trusted_aggregator)) for name, param in model.named_parameters())\n",
    "for i_epoch in range(n_epochs):\n",
    "    print(\"Epoch {:d}:\".format(i_epoch))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    model = model.send(syft.local_worker)\n",
    "    # Track the number of training examples in order to average the gradients later (+ additional stats)\n",
    "    instance_count = torch.tensor(0.).send(trusted_aggregator)\n",
    "    running_loss   = torch.tensor(0.).send(trusted_aggregator)\n",
    "    correct_count  = torch.tensor(0.).send(trusted_aggregator)\n",
    "    for i, worker in enumerate(workers):\n",
    "        print(\"worker {:d}/{:d}\".format(i, len(workers)-1), end='\\r')\n",
    "        model.move(worker)\n",
    "\n",
    "        dataset = federated_mnist_trainset.datasets[worker.id]\n",
    "        dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        imgs, labels = next(iter(dataloader))\n",
    "\n",
    "        preds = model(imgs)\n",
    "\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        current_instance_count = torch.tensor(imgs.shape[0]).send(worker)\n",
    "        current_loss           = loss.data.clone()\n",
    "        current_correct_count  = torch.sum(torch.eq(preds.data.argmax(dim=1), labels))\n",
    "        \n",
    "        model.move(trusted_aggregator)\n",
    "        current_instance_count.move(trusted_aggregator)\n",
    "        current_loss.move(trusted_aggregator)\n",
    "        current_correct_count.move(trusted_aggregator)\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            stored_grads[name].add_(param.grad)\n",
    "            param.grad.zero_()\n",
    "        instance_count.add_(current_instance_count)\n",
    "        running_loss.add_(current_loss)\n",
    "        correct_count.add_(current_correct_count)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        param.grad.add_(torch.div(stored_grads[name], instance_count))\n",
    "        stored_grads[name].zero_()\n",
    "\n",
    "    model = model.get()\n",
    "\n",
    "    avg_loss     = running_loss.div_(instance_count).get().item()\n",
    "    avg_accuracy = correct_count.div_(instance_count).get().item()\n",
    "    \n",
    "    print()\n",
    "    print(\"    Training Loss:\", avg_loss)\n",
    "    print(\"    Training Accuracy:\", avg_accuracy)\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:13:19.170948Z",
     "start_time": "2019-07-05T16:13:16.113946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/10\n",
      "Test Loss: 0.09841546325683594\n",
      "Test Accuracy: 0.9674\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = data.DataLoader(mnist_testset, batch_size=1024)\n",
    "\n",
    "test_loss      = 0\n",
    "instance_count = 0\n",
    "correct_count  = 0\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(test_dataloader, 1):\n",
    "        print(\"Batch {:d}/{:d}\".format(i, len(test_dataloader)), end='\\r')\n",
    "        instance_count += imgs.size(0)\n",
    "\n",
    "        preds = model(imgs)\n",
    "\n",
    "        test_loss += criterion(preds, labels).item()\n",
    "        correct_count += (preds.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "print()\n",
    "print(\"Test Loss:\", test_loss / instance_count)\n",
    "print(\"Test Accuracy:\", correct_count / instance_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged Model Parameters Approach (Based on Andrew's Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:13:19.182916Z",
     "start_time": "2019-07-05T16:13:19.175935Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MNISTClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.757875Z",
     "start_time": "2019-07-05T16:13:19.186907Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "worker 29/29 - Step 29/29 Loss=2.2935       \n",
      "    Test Loss: 2.2980876850128173\n",
      "    Test Accuracy: 0.0868\n",
      "Epoch 1:\n",
      "worker 29/29 - Step 29/29 Loss=2.3009       \n",
      "    Test Loss: 2.294271186828613\n",
      "    Test Accuracy: 0.1528\n",
      "Epoch 2:\n",
      "worker 29/29 - Step 29/29 Loss=2.2947       \n",
      "    Test Loss: 2.289651180267334\n",
      "    Test Accuracy: 0.1844\n",
      "Epoch 3:\n",
      "worker 29/29 - Step 29/29 Loss=2.2800       \n",
      "    Test Loss: 2.2833085285186767\n",
      "    Test Accuracy: 0.2093\n",
      "Epoch 4:\n",
      "worker 29/29 - Step 29/29 Loss=2.2648       \n",
      "    Test Loss: 2.2742213241577147\n",
      "    Test Accuracy: 0.24\n",
      "Epoch 5:\n",
      "worker 29/29 - Step 29/29 Loss=2.2386       \n",
      "    Test Loss: 2.259975256729126\n",
      "    Test Accuracy: 0.2531\n",
      "Epoch 6:\n",
      "worker 29/29 - Step 29/29 Loss=2.2235       \n",
      "    Test Loss: 2.2355145835876464\n",
      "    Test Accuracy: 0.2522\n",
      "Epoch 7:\n",
      "worker 29/29 - Step 29/29 Loss=2.1236       \n",
      "    Test Loss: 2.187098441696167\n",
      "    Test Accuracy: 0.2788\n",
      "Epoch 8:\n",
      "worker 29/29 - Step 29/29 Loss=1.9480       \n",
      "    Test Loss: 2.071344761657715\n",
      "    Test Accuracy: 0.3846\n",
      "Epoch 9:\n",
      "worker 29/29 - Step 29/29 Loss=1.5123       \n",
      "    Test Loss: 1.7592478303909302\n",
      "    Test Accuracy: 0.5782\n"
     ]
    }
   ],
   "source": [
    "n_epochs   = 10\n",
    "n_steps    = 30\n",
    "lr         = 2e-2\n",
    "batch_size = 128\n",
    "\n",
    "local_models     = [model.copy().send(worker) for worker in workers]\n",
    "local_optimizers = [optim.SGD(local_model.parameters(), lr=lr) for local_model in local_models]\n",
    "criterion        = nn.CrossEntropyLoss()\n",
    "test_dataloader  = data.DataLoader(mnist_testset, batch_size=1024)\n",
    "\n",
    "for i_epoch in range(n_epochs):\n",
    "    print(\"Epoch {:d}:\".format(i_epoch))\n",
    "\n",
    "    for i, (worker, local_model, local_optimizer) in enumerate(zip(workers, local_models, local_optimizers)):\n",
    "\n",
    "        dataset = federated_mnist_trainset.datasets[worker.id]\n",
    "        dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        loss = float('inf')\n",
    "        for i_step in range(n_steps):\n",
    "            print(\"worker {:d}/{:d} - Step {:d}/{:d} Loss={:.4f}       \".format(i, len(workers)-1, i_step, n_steps-1, loss), end='\\r')\n",
    "\n",
    "            imgs, labels = next(iter(dataloader))\n",
    "\n",
    "            preds = local_model(imgs)\n",
    "\n",
    "            local_optimizer.zero_grad()\n",
    "            loss = criterion(preds, labels)\n",
    "            loss.backward()\n",
    "            local_optimizer.step()\n",
    "            loss = loss.data.clone().get().item()\n",
    "\n",
    "    model = model.send(trusted_aggregator)\n",
    "    for global_param, local_params in zip(model.parameters(), zip(*[local_model.parameters() for local_model in local_models])):\n",
    "        debug = global_param.data.clone().get()\n",
    "        global_param.data.zero_()\n",
    "        global_param.data.add_(torch.mean(torch.stack([local_param.data.clone().move(trusted_aggregator) for local_param in local_params]), dim=0))\n",
    "        for local_param, worker in zip(local_params, workers):\n",
    "            local_param.data.set_(global_param.data.clone().move(worker))\n",
    "\n",
    "    model = model.get()\n",
    "\n",
    "    test_loss      = 0\n",
    "    instance_count = 0\n",
    "    correct_count  = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_dataloader:\n",
    "            instance_count += imgs.size(0)\n",
    "\n",
    "            preds = model(imgs)\n",
    "\n",
    "            test_loss += criterion(preds, labels).item() * imgs.size(0)\n",
    "            correct_count += (preds.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    print()\n",
    "    print(\"    Test Loss:\", test_loss / instance_count)\n",
    "    print(\"    Test Accuracy:\", correct_count / instance_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Intro to Additive Secret Sharing\n",
    "\n",
    "While being able to have a trusted third party to perform the aggregation is certainly nice, in an ideal setting we wouldn't have to trust anyone at all. This is where Cryptography can provide an interesting alterantive. \n",
    "\n",
    "Specifically, we're going to be looking at a simple protocol for Secure Multi-Party Computation called Additive Secret Sharing. This protocol will allow multiple parties (of size 3 or more) to aggregate their gradients without the use of a trusted 3rd party to perform the aggregation. In other words, we can add 3 numbers together from 3 different people without anyone ever learning the inputs of any other actors.\n",
    "\n",
    "Let's start by considering the number 5, which we'll put into a varible x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.764882Z",
     "start_time": "2019-07-05T16:49:07.760866Z"
    }
   },
   "outputs": [],
   "source": [
    "x = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we wanted to SHARE the ownership of this number between two people, Alice and Bob. We could split this number into two shares, 2, and 3, and give one to Alice and one to Bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.782859Z",
     "start_time": "2019-07-05T16:49:07.767410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "bob_x_share = 2\n",
    "alice_x_share = 3\n",
    "\n",
    "decrypted_x = bob_x_share + alice_x_share\n",
    "print(decrypted_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that neither Bob nor Alice know the value of x. They only know the value of their own SHARE of x. Thus, the true value of X is hidden (i.e., encrypted). \n",
    "\n",
    "The truly amazing thing, however, is that Alice and Bob can still compute using this value! They can perform arithmetic over the hidden value! Let's say Bob and Alice wanted to multiply this value by 2! If each of them multiplied their respective share by 2, then the hidden number between them is also multiplied! Check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.797820Z",
     "start_time": "2019-07-05T16:49:07.785853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "bob_x_share = 2 * 2\n",
    "alice_x_share = 3 * 2\n",
    "\n",
    "decrypted_x = bob_x_share + alice_x_share\n",
    "print(decrypted_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This even works for addition between two shared values!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.814775Z",
     "start_time": "2019-07-05T16:49:07.801809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# encrypted \"5\"\n",
    "bob_x_share = 2\n",
    "alice_x_share = 3\n",
    "\n",
    "# encrypted \"7\"\n",
    "bob_y_share = 5\n",
    "alice_y_share = 2\n",
    "\n",
    "# encrypted 5 + 7\n",
    "bob_z_share = bob_x_share + bob_y_share\n",
    "alice_z_share = alice_x_share + alice_y_share\n",
    "\n",
    "decrypted_z = bob_z_share + alice_z_share\n",
    "print(decrypted_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we just added two numbers together while they were still encrypted!!!\n",
    "\n",
    "One small tweak - notice that since all our numbers are positive, it's possible for each share to reveal a little bit of information about the hidden value, namely, it's always greater than the share. Thus, if Bob has a share \"3\" then he knows that the encrypted value is at least 3.\n",
    "\n",
    "This would be quite bad, but can be solved through a simple fix. Decryption happens by summing all the shares together MODULUS some constant. I.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.828736Z",
     "start_time": "2019-07-05T16:49:07.817767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23740629843736686616461\n"
     ]
    }
   ],
   "source": [
    "x = 5\n",
    "\n",
    "Q = 23740629843760239486723\n",
    "\n",
    "bob_x_share = 23552870267 # <- a random number\n",
    "alice_x_share = Q - bob_x_share + x\n",
    "print(alice_x_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.840705Z",
     "start_time": "2019-07-05T16:49:07.832725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print((bob_x_share + alice_x_share) % Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, as you can see, both shares are wildly larger than the number being shared, meaning that individual shares no longer leak this inforation. However, all the properties we discussed earlier still hold! (addition, encryption, decryption, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Build Methods for Encrypt, Decrypt, and Add \n",
    "\n",
    "In this project, you must take the lessons we learned in the last section and write general methods for encrypt, decrypt, and add. Store shares for a variable in a tuple like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.849681Z",
     "start_time": "2019-07-05T16:49:07.843697Z"
    }
   },
   "outputs": [],
   "source": [
    "x_share = (2,5,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though normally those shares would be distributed amongst several workers, you can store them in ordered tuples like this for now :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.863645Z",
     "start_time": "2019-07-05T16:49:07.853671Z"
    }
   },
   "outputs": [],
   "source": [
    "Q = 23740629843760239486723\n",
    "\n",
    "def encrypt(x, n_shares):\n",
    "    assert x < Q, \"`x` is too large.\"\n",
    "    shares = []\n",
    "\n",
    "    for _ in range(n_shares - 1):\n",
    "        shares.append(random.randint(0, Q))\n",
    "\n",
    "    shares.append(Q - (sum(shares) % Q) + x)\n",
    "\n",
    "#     random.shuffle(shares)\n",
    "\n",
    "    return tuple(shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.891576Z",
     "start_time": "2019-07-05T16:49:07.867635Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17411921852787415668783,\n",
       " 9495176865499091630711,\n",
       " 17880416036434995975129,\n",
       " 9896138474794586231777,\n",
       " 16538236301764628953776)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encrypt(7, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.901595Z",
     "start_time": "2019-07-05T16:49:07.895611Z"
    }
   },
   "outputs": [],
   "source": [
    "def decrypt(shares):\n",
    "    return sum(shares) % Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.913568Z",
     "start_time": "2019-07-05T16:49:07.905584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decrypt(encrypt(7, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.925044Z",
     "start_time": "2019-07-05T16:49:07.917558Z"
    }
   },
   "outputs": [],
   "source": [
    "def add(shares_a, shares_b):\n",
    "    assert len(shares_a) == len(shares_b), \"Length mismatch between two shares.\"\n",
    "\n",
    "    return tuple(a + b for a, b in zip(shares_a, shares_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.938009Z",
     "start_time": "2019-07-05T16:49:07.928035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decrypt(add(encrypt(173, 10), encrypt(141, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Intro to Fixed Precision Encoding\n",
    "\n",
    "As you may remember, our goal is to aggregate gradients using this new Secret Sharing technique. However, the protocol we've just explored in the last section uses positive integers. However, our neural network weights are NOT integers. Instead, our weights are decimals (floating point numbers).\n",
    "\n",
    "Not a huge deal! We just need to use a fixed precision encoding, which lets us do computation over decimal numbers using integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.951972Z",
     "start_time": "2019-07-05T16:49:07.946986Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE=10\n",
    "PRECISION=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.966931Z",
     "start_time": "2019-07-05T16:49:07.957957Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode(x):\n",
    "    return int((x * (BASE ** PRECISION)) % Q)\n",
    "\n",
    "def decode(x):\n",
    "    return (x if x <= Q/2 else x - Q) / BASE**PRECISION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.979896Z",
     "start_time": "2019-07-05T16:49:07.971917Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:07.989869Z",
     "start_time": "2019-07-05T16:49:07.982889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(35000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:49:42.108725Z",
     "start_time": "2019-07-05T16:49:42.101707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = encrypt(encode(5.5), 3)\n",
    "y = encrypt(encode(2.3), 3)\n",
    "z = add(x,y)\n",
    "decode(decrypt(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson: Secret Sharing + Fixed Precision in PySyft\n",
    "\n",
    "While writing things from scratch is certainly educational, PySyft makes a great deal of this much easier for us through its abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:57:50.353118Z",
     "start_time": "2019-07-05T16:57:50.347133Z"
    }
   },
   "outputs": [],
   "source": [
    "bob = syft.VirtualWorker(hook, id='bob')\n",
    "alice = syft.VirtualWorker(hook, id='alice')\n",
    "secure_worker = syft.VirtualWorker(hook, id='secure_worker')\n",
    "\n",
    "bob = bob.clear_objects()\n",
    "alice = alice.clear_objects()\n",
    "secure_worker = secure_worker.clear_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:57:52.099639Z",
     "start_time": "2019-07-05T16:57:52.095650Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secret Sharing Using PySyft\n",
    "\n",
    "We can share using the simple .share() method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:58:35.874297Z",
     "start_time": "2019-07-05T16:58:35.859821Z"
    }
   },
   "outputs": [],
   "source": [
    "x = x.share(bob, alice, secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:58:52.366839Z",
     "start_time": "2019-07-05T16:58:52.360366Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{52004417206: tensor([4607945012089164331,  933574258170101832, 3364821117505162012,\n",
       "         2837592453351812753, 1788268197691109352])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob._objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and as you can see, Bob now has one of the shares of x! Furthermore, we can still call addition in this state, and PySyft will automatically perform the remote execution for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:59:05.319563Z",
     "start_time": "2019-07-05T16:59:05.312581Z"
    }
   },
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:59:09.225333Z",
     "start_time": "2019-07-05T16:59:09.220349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[AdditiveSharingTensor]\n",
       "\t-> (Wrapper)>[PointerTensor | me:39716819677 -> bob:87758319155]\n",
       "\t-> (Wrapper)>[PointerTensor | me:53980983548 -> alice:52966939567]\n",
       "\t-> (Wrapper)>[PointerTensor | me:8098695900 -> secure_worker:45927138476]\n",
       "\t*crypto provider: me*"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:59:48.837294Z",
     "start_time": "2019-07-05T16:59:48.827320Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Precision using PySyft\n",
    "\n",
    "We can also convert a tensor to fixed precision using .fix_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:59:55.728945Z",
     "start_time": "2019-07-05T16:59:55.723958Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([0.1,0.2,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T16:59:56.484572Z",
     "start_time": "2019-07-05T16:59:56.458544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000, 0.3000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T17:00:00.034942Z",
     "start_time": "2019-07-05T17:00:00.030956Z"
    }
   },
   "outputs": [],
   "source": [
    "x = x.fix_prec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T17:00:00.491127Z",
     "start_time": "2019-07-05T17:00:00.484770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100, 200, 300])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.child.child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T17:00:06.984413Z",
     "start_time": "2019-07-05T17:00:06.980440Z"
    }
   },
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T17:00:11.526316Z",
     "start_time": "2019-07-05T17:00:11.518830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.4000, 0.6000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.float_prec()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Fixed Precision\n",
    "\n",
    "And of course, we can combine the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T17:00:17.618106Z",
     "start_time": "2019-07-05T17:00:17.614099Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([0.1, 0.2, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T17:00:18.934983Z",
     "start_time": "2019-07-05T17:00:18.928960Z"
    }
   },
   "outputs": [],
   "source": [
    "x = x.fix_prec().share(bob, alice, secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T17:00:20.247215Z",
     "start_time": "2019-07-05T17:00:20.241872Z"
    }
   },
   "outputs": [],
   "source": [
    "y = x + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T17:00:21.323465Z",
     "start_time": "2019-07-05T17:00:21.314757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.4000, 0.6000])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.get().float_prec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to make the point that people can see the model averages in the clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Federated Learning with Encrypted Gradient Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

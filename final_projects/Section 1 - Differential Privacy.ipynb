{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Project:\n",
    "\n",
    "For the final project for this section, you're going to train a DP model using this PATE method on the MNIST dataset, provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:30:30.984840Z",
     "start_time": "2019-06-21T13:30:27.322463Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.13.1). Fix this by compiling custom ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dists\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST Training & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:30:31.142569Z",
     "start_time": "2019-06-21T13:30:30.987843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 60000\n",
      "Test Set Size: 10000\n",
      "\n",
      "Min Data Value: tensor(0, dtype=torch.uint8)\n",
      "Max Data Value: tensor(255, dtype=torch.uint8)\n",
      "\n",
      "Train Label Counts: {0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n",
      "Test Label Counts: {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root='../data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "print(\"Training Set Size:\", len(mnist_trainset))\n",
    "print(\"Test Set Size:\", len(mnist_testset))\n",
    "print()\n",
    "print(\"Min Data Value:\", torch.min(mnist_trainset.data.min(), mnist_testset.data.min()))\n",
    "print(\"Max Data Value:\", torch.max(mnist_trainset.data.max(), mnist_testset.data.max()))\n",
    "print()\n",
    "print(\"Train Label Counts:\", {label.item():count.item() for label, count in zip(*torch.unique(mnist_trainset.targets, return_counts=True))})\n",
    "print(\"Test Label Counts:\", {label.item():count.item() for label, count in zip(*torch.unique(mnist_testset.targets, return_counts=True))})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:30:31.167369Z",
     "start_time": "2019-06-21T13:30:31.144551Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each teacher dataset: 240\n",
      "Size of the (soon to be) differentially private dataset: 9000\n",
      "Size of the test dataset: 1000\n"
     ]
    }
   ],
   "source": [
    "n_teachers = 250\n",
    "batch_size = 30\n",
    "\n",
    "_teacher_dataset_len = len(mnist_trainset) // n_teachers\n",
    "\n",
    "teacher_datasets = [data.Subset(mnist_trainset, list(range(i*_teacher_dataset_len, (i+1)*_teacher_dataset_len))) for i in range(n_teachers)]\n",
    "dp_dataset       = data.Subset(mnist_testset, list(range(int(len(mnist_testset) * 0.9))))\n",
    "test_dataset     = data.Subset(mnist_testset, list(range(int(len(mnist_testset) * 0.9), len(mnist_testset))))\n",
    "\n",
    "teacher_dataloaders = [data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True) for dataset in teacher_datasets]\n",
    "dp_dataloader       = data.DataLoader(dp_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader     = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "print(\"Size of each teacher dataset:\", _teacher_dataset_len)\n",
    "print(\"Size of the (soon to be) differentially private dataset:\", len(dp_dataset))\n",
    "print(\"Size of the test dataset:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:30:31.186216Z",
     "start_time": "2019-06-21T13:30:31.172861Z"
    }
   },
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        \n",
    "        # 1x28x28\n",
    "        self.bn0        = nn.BatchNorm2d(1)\n",
    "        self.conv0      = nn.Conv2d(1, 4, 3, padding=1)\n",
    "        self.bn1        = nn.BatchNorm2d(4)\n",
    "        self.maxpool0   = nn.MaxPool2d(2)\n",
    "        # 4x14x14\n",
    "        self.conv1      = nn.Conv2d(4, 8, 3, padding=1)\n",
    "        self.bn2        = nn.BatchNorm2d(8)\n",
    "        self.maxpool1   = nn.MaxPool2d(2)\n",
    "        # 8x 7x 7\n",
    "        self.conv2      = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        self.maxpool2   = nn.MaxPool2d(2, padding=1)\n",
    "        # 16x4x 4 = 256\n",
    "        self.bn3        = nn.BatchNorm1d(256)\n",
    "        self.fc         = nn.Linear(256, 10)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.conv0(self.bn0(x)))\n",
    "        x = self.maxpool0(x)\n",
    "        x = self.activation(self.conv1(self.bn1(x)))\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.activation(self.conv2(self.bn2(x)))\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.fc(self.bn3(x.view(-1, 256)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Teacher & DF Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T13:30:34.686104Z",
     "start_time": "2019-06-21T13:30:31.187740Z"
    }
   },
   "outputs": [],
   "source": [
    "teachers = [MNISTClassifier().to(device) for _ in range(n_teachers)]\n",
    "dp_model = MNISTClassifier().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-06-21T13:30:37.833Z"
    },
    "code_folding": [
     10
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher 27/249: (2/7)\r"
     ]
    }
   ],
   "source": [
    "lr       = 1e-2\n",
    "n_epochs = 5\n",
    "\n",
    "teacher_optimizers = [optim.Adam(model.parameters(), lr=lr) for model in teachers]\n",
    "criterion          = nn.CrossEntropyLoss()\n",
    "\n",
    "for model in teachers:\n",
    "    model.train()\n",
    "\n",
    "teachers_train_history = {'avg_losses':{}, 'avg_accuracies': {}}\n",
    "for i_epoch in range(n_epochs):\n",
    "    avg_losses      = []\n",
    "    avg_accuracies  = []\n",
    "\n",
    "    for i_model in range(n_teachers):\n",
    "        instance_count = 0\n",
    "        total_loss     = 0.\n",
    "        correct_count  = 0\n",
    "\n",
    "        model      = teachers[i_model]\n",
    "        dataloader = teacher_dataloaders[i_model]\n",
    "        optimizer  = teacher_optimizers[i_model]\n",
    "\n",
    "        n_batches = len(dataloader)\n",
    "        _prev_str_len = 0\n",
    "        for i, (imgs, labels) in enumerate(dataloader):\n",
    "            _batch_str = \"Teacher {:d}/{:d}: ({:d}/{:d})\".format(i_model, n_teachers-1, i, n_batches-1)\n",
    "            print(_batch_str + ' ' * (_prev_str_len - len(_batch_str)), end='\\r')\n",
    "            _prev_str_len = len(_batch_str)\n",
    "\n",
    "            instance_count += imgs.size(0)\n",
    "\n",
    "            imgs   = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outs  = model(imgs)\n",
    "            preds = torch.argmax(outs, dim=1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            \n",
    "            correct_count += (preds == labels).sum().item()\n",
    "\n",
    "        avg_losses.append(total_loss / instance_count)\n",
    "        avg_accuracies.append(correct_count / instance_count)\n",
    "\n",
    "    _epoch_str = \"Epoch {:d}/{:d}\".format(i_epoch, n_epochs-1)\n",
    "    _epoch_str += ' ' * (_prev_str_len - len(_epoch_str))\n",
    "    print(_epoch_str)\n",
    "    print(\"    Avg Losses:\", [round(avg_loss, 5) for avg_loss in avg_losses])\n",
    "    print(\"    Avg Accuracies:\", [round(avg_acc, 4) for avg_acc in avg_accuracies])\n",
    "    print()\n",
    "\n",
    "    teachers_train_history['avg_losses'][i_epoch]     = avg_losses\n",
    "    teachers_train_history['avg_accuracies'][i_epoch] = avg_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Labels for the Differentially Private Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Labels Using Teacher Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T02:46:18.649729Z",
     "start_time": "2019-06-21T02:46:14.624719Z"
    },
    "code_folding": [
     9
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 35/35 - Teacher 19/19\n",
      "tensor([[7, 2, 1,  ..., 6, 9, 0],\n",
      "        [7, 2, 1,  ..., 6, 9, 0],\n",
      "        [7, 2, 1,  ..., 6, 9, 0],\n",
      "        ...,\n",
      "        [7, 2, 1,  ..., 6, 9, 0],\n",
      "        [7, 2, 1,  ..., 6, 9, 0],\n",
      "        [7, 2, 1,  ..., 6, 9, 0]])\n"
     ]
    }
   ],
   "source": [
    "for model in teachers:\n",
    "    model.eval()\n",
    "\n",
    "dataloader = data.DataLoader(dp_dataset, batch_size=256, shuffle=False, drop_last=False)\n",
    "\n",
    "batches_of_preds = []\n",
    "\n",
    "n_batches = len(dataloader)\n",
    "_prev_str_len = 0\n",
    "for i, (imgs, _) in enumerate(dataloader):\n",
    "    imgs = imgs.to(device)\n",
    "\n",
    "    batch_of_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for j, model in enumerate(teachers):\n",
    "            _progress_str = \"Batch {:d}/{:d} - Teacher {:d}/{:d}\".format(i, n_batches-1, j, n_teachers-1)\n",
    "            print(_progress_str + ' ' * (_prev_str_len - len(_progress_str)), end='\\r')\n",
    "            _prev_str_len = len(_progress_str)\n",
    "\n",
    "            outs  = model(imgs)\n",
    "            preds = outs.argmax(dim=1)\n",
    "            batch_of_preds.append(preds.cpu())\n",
    "    \n",
    "    batches_of_preds.append(batch_of_preds)\n",
    "        \n",
    "label_preds = torch.cat(\n",
    "    [torch.stack([preds for preds in batch_of_preds], dim=0)\n",
    "     for batch_of_preds in batches_of_preds],\n",
    "    dim=1)\n",
    "\n",
    "print()\n",
    "print(label_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-20T16:40:06.332084Z",
     "start_time": "2019-06-20T16:40:06.325637Z"
    }
   },
   "source": [
    "### Get Label Counts for Each Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T02:46:31.881245Z",
     "start_time": "2019-06-21T02:46:31.842582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  0,  0,  ...,  0,  0, 20],\n",
      "        [ 0,  0, 20,  ...,  0,  0,  0],\n",
      "        [ 0, 20,  0,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [20,  0,  0,  ...,  0,  2,  0],\n",
      "        [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "        [ 0,  0,  0,  ...,  0, 18,  0]])\n"
     ]
    }
   ],
   "source": [
    "label_counts = torch.from_numpy(np.apply_along_axis(lambda x: np.bincount(x, minlength=10), axis=0, arr=label_preds.numpy()))\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Labels from Noisy Counts with a Certain $\\epsilon$ Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T03:13:01.956805Z",
     "start_time": "2019-06-21T03:13:01.938957Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1,  ..., 6, 9, 0])\n",
      "\n",
      "Noisy Accuracy Against Predictions: 0.9543333053588867\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.3\n",
    "\n",
    "noise_dist = dists.Laplace(loc=torch.zeros([], dtype=torch.float),\n",
    "                           scale=torch.full([], 1 / epsilon, dtype=torch.float))\n",
    "\n",
    "noisy_counts = label_counts.float() + noise_dist.sample([10, label_counts.size(1)])\n",
    "\n",
    "generated_labels = noisy_counts.argmax(dim=0)\n",
    "print(generated_labels)\n",
    "print()\n",
    "print(\"Noisy Accuracy Against Predictions:\", (generated_labels == label_counts.argmax(dim=0)).float().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PATE Analysis to Check Information Leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-21T05:23:06.724816Z",
     "start_time": "2019-06-21T03:13:17.916752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: May not have used enough values of l. Increase 'moments' variable and run again.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3251.512925465141, 3251.51292546497)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pate.perform_analysis(label_preds, generated_labels, epsilon, delta=1e-05, moments=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the Generated Labels to the DP Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_testset.targets[dp_dataset.indices] = generated_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the DP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "lr       = 3e-3\n",
    "n_epochs = 10\n",
    "\n",
    "dp_optimizer = optim.Adam(dp_model.parameters(), lr=lr)\n",
    "criterion    = nn.CrossEntropyLoss()\n",
    "\n",
    "dp_model.train()\n",
    "\n",
    "dp_train_history = {'avg_losses':{}, 'avg_accuracies': {}}\n",
    "for i_epoch in range(n_epochs):\n",
    "    instance_count = 0\n",
    "    total_loss     = 0.\n",
    "    correct_count  = 0.\n",
    "\n",
    "    n_batches = len(dp_dataloader)\n",
    "    _prev_str_len = 0\n",
    "    for i, (imgs, labels) in enumerate(dp_dataloader):\n",
    "        _batch_str = \"Epoch {:d}/{:d}: ({:d}/{:d})\".format(i_epoch, n_epochs-1, i, n_batches-1)\n",
    "        print(_batch_str + ' ' * (_prev_str_len - len(_batch_str)), end='\\r')\n",
    "        _prev_str_len = len(_batch_str)\n",
    "\n",
    "        instance_count += imgs.size(0)\n",
    "\n",
    "        imgs   = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outs  = dp_model(imgs)\n",
    "        preds = torch.argmax(outs, dim=1)\n",
    "\n",
    "        dp_optimizer.zero_grad()\n",
    "        loss = criterion(outs, labels)\n",
    "        loss.backward()\n",
    "        dp_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        correct_count += (preds == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / instance_count\n",
    "    avg_accuracy = correct_count / instance_count\n",
    "\n",
    "    print()\n",
    "    print(\"    Avg Loss: {:.6f}\".format(avg_loss))\n",
    "    print(\"    Avg Accuracy: {:.4f}\".format(avg_accuracy))\n",
    "    print()\n",
    "\n",
    "    dp_train_history['avg_losses'][i_epoch]     = avg_loss\n",
    "    dp_train_history['avg_accuracies'][i_epoch] = avg_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Result Model on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     9
    ]
   },
   "outputs": [],
   "source": [
    "dp_model.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "instance_count = 0\n",
    "total_loss     = 0.\n",
    "correct_count  = 0.\n",
    "\n",
    "n_batches = len(dataloader)\n",
    "for i, (imgs, labels) in enumerate(test_dataloader):\n",
    "    print(\"Batch {:d}/{:d}\".format(i, n_batches-1), end='\\r')\n",
    "\n",
    "    instance_count += imgs.size(0)\n",
    "    \n",
    "    imgs = imgs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outs  = model(imgs)\n",
    "    \n",
    "    total_loss += criterion(outs, labels).item()\n",
    "\n",
    "    preds = outs.argmax(dim=1)\n",
    "    \n",
    "    correct_count += (preds == labels).sum().item()\n",
    "\n",
    "print()\n",
    "print(\"Average Loss:\", total_loss / instance_count)\n",
    "print(\"Average Accuracy:\", correct_count / instance_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare Data\n",
    "    1. Split the training dataset into `n + 1` smaller datasets where `n` is the number of teacher models\n",
    "    2. Define a Dataset class and a DataLoader that can give batches of data for all `n` teacher datasets\n",
    "2. Define Model(s)\n",
    "    1. A simple ConvNet for both the main model and all teacher models\n",
    "    2. If too slow: custom `nn.Module` that can process `n` batches at once for all `n` teachers\n",
    "3. Train Teachers\n",
    "4. Label Unlabeled Training Dataset in a Differentially Private Manner\n",
    "    1. Generate raw labels\n",
    "    2. PATE analysis to find a proper `epsilon` value\n",
    "    3. Add proper noise to the label counts\n",
    "    4. Take the labels with most counts\n",
    "5. Train the Main Model On the Training Dataset with Generated Labels\n",
    "6. Test on the Test Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

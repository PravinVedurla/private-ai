{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Project: Federated Learning\n",
    "\n",
    "For the final project for this section, you're going to train a model on the MNIST dataset distributed across multiple devices **without retrieving the raw gradients to the local machine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:51:47.830321Z",
     "start_time": "2019-06-29T14:51:44.958710Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.13.1). Fix this by compiling custom ops.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dists\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import syft\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "hook = syft.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:51:47.841785Z",
     "start_time": "2019-06-29T14:51:47.834787Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<VirtualWorker id:Worker:0 #tensors:0>, <VirtualWorker id:Worker:1 #tensors:0>, <VirtualWorker id:Worker:2 #tensors:0>, <VirtualWorker id:Worker:3 #tensors:0>, <VirtualWorker id:Worker:4 #tensors:0>, <VirtualWorker id:Worker:5 #tensors:0>, <VirtualWorker id:Worker:6 #tensors:0>, <VirtualWorker id:Worker:7 #tensors:0>, <VirtualWorker id:Worker:8 #tensors:0>, <VirtualWorker id:Worker:9 #tensors:0>, <VirtualWorker id:Worker:10 #tensors:0>, <VirtualWorker id:Worker:11 #tensors:0>, <VirtualWorker id:Worker:12 #tensors:0>, <VirtualWorker id:Worker:13 #tensors:0>, <VirtualWorker id:Worker:14 #tensors:0>, <VirtualWorker id:Worker:15 #tensors:0>, <VirtualWorker id:Worker:16 #tensors:0>, <VirtualWorker id:Worker:17 #tensors:0>, <VirtualWorker id:Worker:18 #tensors:0>, <VirtualWorker id:Worker:19 #tensors:0>, <VirtualWorker id:Worker:20 #tensors:0>, <VirtualWorker id:Worker:21 #tensors:0>, <VirtualWorker id:Worker:22 #tensors:0>, <VirtualWorker id:Worker:23 #tensors:0>, <VirtualWorker id:Worker:24 #tensors:0>, <VirtualWorker id:Worker:25 #tensors:0>, <VirtualWorker id:Worker:26 #tensors:0>, <VirtualWorker id:Worker:27 #tensors:0>, <VirtualWorker id:Worker:28 #tensors:0>, <VirtualWorker id:Worker:29 #tensors:0>]\n"
     ]
    }
   ],
   "source": [
    "n_workers = 30\n",
    "\n",
    "workers = [syft.VirtualWorker(hook, id=\"Worker:{:d}\".format(i)) for i in range(n_workers)]\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST Training & Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:51:47.888356Z",
     "start_time": "2019-06-29T14:51:47.844709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 60000\n",
      "Test Set Size: 10000\n"
     ]
    }
   ],
   "source": [
    "mnist_trainset = datasets.MNIST(root='../data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_testset  = datasets.MNIST(root='../data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "print(\"Training Set Size:\", len(mnist_trainset))\n",
    "print(\"Test Set Size:\", len(mnist_testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Federated Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:51:55.950516Z",
     "start_time": "2019-06-29T14:51:48.024410Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FederatedDataset\n",
      "    Distributed accross: Worker:0, Worker:1, Worker:2, Worker:3, Worker:4, Worker:5, Worker:6, Worker:7, Worker:8, Worker:9, Worker:10, Worker:11, Worker:12, Worker:13, Worker:14, Worker:15, Worker:16, Worker:17, Worker:18, Worker:19, Worker:20, Worker:21, Worker:22, Worker:23, Worker:24, Worker:25, Worker:26, Worker:27, Worker:28, Worker:29\n",
      "    Number of datapoints: 60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "federated_mnist_trainset = mnist_trainset.federate(workers)\n",
    "print(federated_mnist_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:51:55.963448Z",
     "start_time": "2019-06-29T14:51:55.952996Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        \n",
    "        ### Batch Normalization layers are incompatible with PySyft :(\n",
    "        # 1x28x28\n",
    "#         self.bn0        = nn.BatchNorm2d(1)\n",
    "        self.conv0      = nn.Conv2d(1, 4, 3, padding=1)\n",
    "#         self.bn1        = nn.BatchNorm2d(4)\n",
    "        self.maxpool0   = nn.MaxPool2d(2)\n",
    "        # 4x14x14\n",
    "        self.conv1      = nn.Conv2d(4, 6, 3, padding=1)\n",
    "#         self.bn2        = nn.BatchNorm2d(6)\n",
    "        self.maxpool1   = nn.MaxPool2d(2)\n",
    "        # 6x 7x 7\n",
    "        self.conv2      = nn.Conv2d(6, 8, 3, padding=1)\n",
    "#         self.bn3        = nn.BatchNorm2d(8)\n",
    "        self.maxpool2   = nn.MaxPool2d(2, padding=1)\n",
    "        # 8x 4x 4 = 128\n",
    "        self.fc         = nn.Linear(128, 10)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = self.bn0(x)\n",
    "        x = self.conv0(x)\n",
    "#         x = self.bn1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool0(x)\n",
    "        x = self.conv1(x)\n",
    "#         x = self.bn2(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "#         x = self.bn3(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.fc(x.view(-1, 128))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:51:55.982996Z",
     "start_time": "2019-06-29T14:51:55.965893Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MNISTClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:54:00.743922Z",
     "start_time": "2019-06-29T14:51:55.986228Z"
    },
    "code_folding": [
     7
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "worker 29/29\n",
      "    Training Loss: 2.306910514831543\n",
      "    Training Accuracy: 0.08124999701976776\n",
      "Epoch 1:\n",
      "worker 29/29\n",
      "    Training Loss: 2.2990634441375732\n",
      "    Training Accuracy: 0.10104166716337204\n",
      "Epoch 2:\n",
      "worker 29/29\n",
      "    Training Loss: 2.2804172039031982\n",
      "    Training Accuracy: 0.22604165971279144\n",
      "Epoch 3:\n",
      "worker 29/29\n",
      "    Training Loss: 2.2394001483917236\n",
      "    Training Accuracy: 0.4260416626930237\n",
      "Epoch 4:\n",
      "worker 29/29\n",
      "    Training Loss: 2.1809535026550293\n",
      "    Training Accuracy: 0.36666667461395264\n",
      "Epoch 5:\n",
      "worker 29/29\n",
      "    Training Loss: 2.061296224594116\n",
      "    Training Accuracy: 0.4645833373069763\n",
      "Epoch 6:\n",
      "worker 29/29\n",
      "    Training Loss: 1.889341115951538\n",
      "    Training Accuracy: 0.47708332538604736\n",
      "Epoch 7:\n",
      "worker 29/29\n",
      "    Training Loss: 1.6500238180160522\n",
      "    Training Accuracy: 0.5583333373069763\n",
      "Epoch 8:\n",
      "worker 29/29\n",
      "    Training Loss: 1.3526575565338135\n",
      "    Training Accuracy: 0.5885416865348816\n",
      "Epoch 9:\n",
      "worker 29/29\n",
      "    Training Loss: 1.1131576299667358\n",
      "    Training Accuracy: 0.6875\n",
      "Epoch 10:\n",
      "worker 29/29\n",
      "    Training Loss: 1.099799394607544\n",
      "    Training Accuracy: 0.6239583492279053\n",
      "Epoch 11:\n",
      "worker 29/29\n",
      "    Training Loss: 0.9254631400108337\n",
      "    Training Accuracy: 0.6739583611488342\n",
      "Epoch 12:\n",
      "worker 29/29\n",
      "    Training Loss: 0.8539355397224426\n",
      "    Training Accuracy: 0.7083333134651184\n",
      "Epoch 13:\n",
      "worker 29/29\n",
      "    Training Loss: 0.8223675489425659\n",
      "    Training Accuracy: 0.7281249761581421\n",
      "Epoch 14:\n",
      "worker 29/29\n",
      "    Training Loss: 0.7094126343727112\n",
      "    Training Accuracy: 0.7749999761581421\n",
      "Epoch 15:\n",
      "worker 29/29\n",
      "    Training Loss: 0.6295483708381653\n",
      "    Training Accuracy: 0.8010416626930237\n",
      "Epoch 16:\n",
      "worker 29/29\n",
      "    Training Loss: 0.6094332337379456\n",
      "    Training Accuracy: 0.8083333373069763\n",
      "Epoch 17:\n",
      "worker 29/29\n",
      "    Training Loss: 0.5677691698074341\n",
      "    Training Accuracy: 0.815625011920929\n",
      "Epoch 18:\n",
      "worker 29/29\n",
      "    Training Loss: 0.5006192922592163\n",
      "    Training Accuracy: 0.8374999761581421\n",
      "Epoch 19:\n",
      "worker 29/29\n",
      "    Training Loss: 0.4674167335033417\n",
      "    Training Accuracy: 0.8416666388511658\n",
      "Epoch 20:\n",
      "worker 29/29\n",
      "    Training Loss: 0.4810832738876343\n",
      "    Training Accuracy: 0.8552083373069763\n",
      "Epoch 21:\n",
      "worker 29/29\n",
      "    Training Loss: 0.49461987614631653\n",
      "    Training Accuracy: 0.856249988079071\n",
      "Epoch 22:\n",
      "worker 29/29\n",
      "    Training Loss: 0.40729814767837524\n",
      "    Training Accuracy: 0.8770833611488342\n",
      "Epoch 23:\n",
      "worker 29/29\n",
      "    Training Loss: 0.4141417145729065\n",
      "    Training Accuracy: 0.8791666626930237\n",
      "Epoch 24:\n",
      "worker 29/29\n",
      "    Training Loss: 0.4117000699043274\n",
      "    Training Accuracy: 0.862500011920929\n",
      "Epoch 25:\n",
      "worker 29/29\n",
      "    Training Loss: 0.3724898397922516\n",
      "    Training Accuracy: 0.8791666626930237\n",
      "Epoch 26:\n",
      "worker 29/29\n",
      "    Training Loss: 0.30299901962280273\n",
      "    Training Accuracy: 0.90625\n",
      "Epoch 27:\n",
      "worker 29/29\n",
      "    Training Loss: 0.37549930810928345\n",
      "    Training Accuracy: 0.887499988079071\n",
      "Epoch 28:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2808297276496887\n",
      "    Training Accuracy: 0.9083333611488342\n",
      "Epoch 29:\n",
      "worker 29/29\n",
      "    Training Loss: 0.32383012771606445\n",
      "    Training Accuracy: 0.893750011920929\n",
      "Epoch 30:\n",
      "worker 29/29\n",
      "    Training Loss: 0.35618171095848083\n",
      "    Training Accuracy: 0.8958333134651184\n",
      "Epoch 31:\n",
      "worker 29/29\n",
      "    Training Loss: 0.31999579071998596\n",
      "    Training Accuracy: 0.9145833253860474\n",
      "Epoch 32:\n",
      "worker 29/29\n",
      "    Training Loss: 0.28228190541267395\n",
      "    Training Accuracy: 0.9145833253860474\n",
      "Epoch 33:\n",
      "worker 29/29\n",
      "    Training Loss: 0.26455751061439514\n",
      "    Training Accuracy: 0.9229166507720947\n",
      "Epoch 34:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2605666220188141\n",
      "    Training Accuracy: 0.925000011920929\n",
      "Epoch 35:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2622615396976471\n",
      "    Training Accuracy: 0.9104166626930237\n",
      "Epoch 36:\n",
      "worker 29/29\n",
      "    Training Loss: 0.309091717004776\n",
      "    Training Accuracy: 0.9104166626930237\n",
      "Epoch 37:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2479097843170166\n",
      "    Training Accuracy: 0.921875\n",
      "Epoch 38:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2507575452327728\n",
      "    Training Accuracy: 0.9260416626930237\n",
      "Epoch 39:\n",
      "worker 29/29\n",
      "    Training Loss: 0.22448137402534485\n",
      "    Training Accuracy: 0.9270833134651184\n",
      "Epoch 40:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2985013425350189\n",
      "    Training Accuracy: 0.9052083492279053\n",
      "Epoch 41:\n",
      "worker 29/29\n",
      "    Training Loss: 0.24619412422180176\n",
      "    Training Accuracy: 0.9260416626930237\n",
      "Epoch 42:\n",
      "worker 29/29\n",
      "    Training Loss: 0.21719200909137726\n",
      "    Training Accuracy: 0.9302083253860474\n",
      "Epoch 43:\n",
      "worker 29/29\n",
      "    Training Loss: 0.23145149648189545\n",
      "    Training Accuracy: 0.9281250238418579\n",
      "Epoch 44:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2157333344221115\n",
      "    Training Accuracy: 0.9427083134651184\n",
      "Epoch 45:\n",
      "worker 29/29\n",
      "    Training Loss: 0.20896805822849274\n",
      "    Training Accuracy: 0.9354166388511658\n",
      "Epoch 46:\n",
      "worker 29/29\n",
      "    Training Loss: 0.18748876452445984\n",
      "    Training Accuracy: 0.9437500238418579\n",
      "Epoch 47:\n",
      "worker 29/29\n",
      "    Training Loss: 0.18548467755317688\n",
      "    Training Accuracy: 0.9479166865348816\n",
      "Epoch 48:\n",
      "worker 29/29\n",
      "    Training Loss: 0.2260051667690277\n",
      "    Training Accuracy: 0.925000011920929\n",
      "Epoch 49:\n",
      "worker 29/29\n",
      "    Training Loss: 0.18543680012226105\n",
      "    Training Accuracy: 0.9416666626930237\n"
     ]
    }
   ],
   "source": [
    "n_epochs   = 50\n",
    "lr         = 2e-2\n",
    "batch_size = 32\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "for i_epoch in range(n_epochs):\n",
    "    print(\"Epoch {:d}:\".format(i_epoch))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    model = model.send(syft.local_worker)\n",
    "    # Track the number of training examples in order to average the gradients later (+ additional stats)\n",
    "    instance_count = torch.tensor(0.).send(syft.local_worker)\n",
    "    running_loss   = torch.tensor(0.).send(syft.local_worker)\n",
    "    correct_count  = torch.tensor(0.).send(syft.local_worker)\n",
    "    for i, worker in enumerate(workers):\n",
    "        print(\"worker {:d}/{:d}\".format(i, len(workers)-1), end='\\r')\n",
    "        model.move(worker)\n",
    "        instance_count.move(worker)\n",
    "        running_loss.move(worker)\n",
    "        correct_count.move(worker)\n",
    "\n",
    "        dataset = federated_mnist_trainset.datasets[worker.id]\n",
    "        dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        imgs, labels = next(iter(dataloader))\n",
    "\n",
    "        instance_count.add_(imgs.shape[0]) # calling `.size()` on a remote tensor returns `Size([0])`, so getting `.shape` instead.\n",
    "        \n",
    "        preds = model(imgs)\n",
    "\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss.add_(loss.data)\n",
    "        correct_count.add_(torch.sum(torch.eq(preds.data.argmax(dim=1), labels)))\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.grad.div_(instance_count)\n",
    "\n",
    "    model = model.get()\n",
    "\n",
    "    avg_loss     = running_loss.div_(instance_count).get().item()\n",
    "    avg_accuracy = correct_count.div_(instance_count).get().item()\n",
    "    \n",
    "    print()\n",
    "    print(\"    Training Loss:\", avg_loss)\n",
    "    print(\"    Training Accuracy:\", avg_accuracy)\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-29T14:54:03.469439Z",
     "start_time": "2019-06-29T14:54:00.747394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/10\n",
      "Test Loss: 0.19153675537109374\n",
      "Test Accuracy: 0.9393\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = data.DataLoader(mnist_testset, batch_size=1024)\n",
    "\n",
    "test_loss      = 0\n",
    "instance_count = 0\n",
    "correct_count  = 0\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(test_dataloader, 1):\n",
    "        print(\"Batch {:d}/{:d}\".format(i, len(test_dataloader)), end='\\r')\n",
    "        instance_count += imgs.size(0)\n",
    "\n",
    "        preds = model(imgs)\n",
    "\n",
    "        test_loss += criterion(preds, labels).item()\n",
    "        correct_count += (preds.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "print()\n",
    "print(\"Test Loss:\", test_loss / instance_count)\n",
    "print(\"Test Accuracy:\", correct_count / instance_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
